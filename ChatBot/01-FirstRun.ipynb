{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/dialogs.txt', sep='\\t', names=['Question', 'Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    # Lower text + removing unwanted characters.\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"[^a-z?.!,:]+\", \" \", x.lower()))\n",
    "\n",
    "    # Separate punctuation to be their own tokens.\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"([?.!,:])\", r\" \\1 \", x).strip())\n",
    "\n",
    "    # Clean duplicate empty spaces \n",
    "    df[col] = df[col].apply(lambda x: re.sub(r'[\" \"]+', \" \", x))\n",
    "\n",
    "    # Adding start and end tokens\n",
    "    df[col] = '<s> ' + df[col] + ' <e>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt; hi , how are you doing ? &lt;e&gt;</td>\n",
       "      <td>&lt;s&gt; i m fine . how about yourself ? &lt;e&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt; i m fine . how about yourself ? &lt;e&gt;</td>\n",
       "      <td>&lt;s&gt; i m pretty good . thanks for asking . &lt;e&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;s&gt; i m pretty good . thanks for asking . &lt;e&gt;</td>\n",
       "      <td>&lt;s&gt; no problem . so how have you been ? &lt;e&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt; no problem . so how have you been ? &lt;e&gt;</td>\n",
       "      <td>&lt;s&gt; i ve been great . what about you ? &lt;e&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt; i ve been great . what about you ? &lt;e&gt;</td>\n",
       "      <td>&lt;s&gt; i ve been good . i m in school right now ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Question  \\\n",
       "0               <s> hi , how are you doing ? <e>   \n",
       "1        <s> i m fine . how about yourself ? <e>   \n",
       "2  <s> i m pretty good . thanks for asking . <e>   \n",
       "3    <s> no problem . so how have you been ? <e>   \n",
       "4     <s> i ve been great . what about you ? <e>   \n",
       "\n",
       "                                              Answer  \n",
       "0            <s> i m fine . how about yourself ? <e>  \n",
       "1      <s> i m pretty good . thanks for asking . <e>  \n",
       "2        <s> no problem . so how have you been ? <e>  \n",
       "3         <s> i ve been great . what about you ? <e>  \n",
       "4  <s> i ve been good . i m in school right now ....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(language):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(language)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def vectorize(tokenizer, language):\n",
    "    tensor = tokenizer.texts_to_sequences(language)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenizer = tokenize(df['Question'])\n",
    "y_tokenizer = tokenize(df['Answer'])\n",
    "\n",
    "x_tensor = vectorize(x_tokenizer, df['Question'])\n",
    "y_tensor = vectorize(y_tokenizer, df['Answer'])\n",
    "\n",
    "max_length_x = x_tensor.shape[1]\n",
    "max_length_y = y_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_tensor, y_tensor, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 24) (64, 24)\n"
     ]
    }
   ],
   "source": [
    "buffer_size = len(x_train)\n",
    "batch_size = 64\n",
    "\n",
    "steps_per_epoch = len(x_train) // batch_size\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(x_tokenizer.word_index) + 1\n",
    "vocab_tar_size = len(y_tokenizer.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "\n",
    "print(example_input_batch.shape, example_target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.enc_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "            )\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)    \n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 24, 1024) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print(sample_output.shape, sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1024) (64, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(attention_result.shape, attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.dec_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2350)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, batch_size)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)\n",
    "\n",
    "print(sample_decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([y_tokenizer.word_index['<s>']] * batch_size, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.152242422103882, Epoch time: 92.29s\n",
      "Epoch 2, loss: 1.8102943897247314, Epoch time: 71.38s\n",
      "Epoch 3, loss: 1.6479251384735107, Epoch time: 72.42s\n",
      "Epoch 4, loss: 1.5498902797698975, Epoch time: 71.32s\n",
      "Epoch 5, loss: 1.4789741039276123, Epoch time: 70.63s\n",
      "Epoch 6, loss: 1.4075080156326294, Epoch time: 70.72s\n",
      "Epoch 7, loss: 1.3519386053085327, Epoch time: 70.27s\n",
      "Epoch 8, loss: 1.298085331916809, Epoch time: 69.35s\n",
      "Epoch 9, loss: 1.2458301782608032, Epoch time: 69.40s\n",
      "Epoch 10, loss: 1.1887924671173096, Epoch time: 69.42s\n",
      "Epoch 11, loss: 1.150111436843872, Epoch time: 70.74s\n",
      "Epoch 12, loss: 1.1070246696472168, Epoch time: 69.79s\n",
      "Epoch 13, loss: 1.0669739246368408, Epoch time: 69.61s\n",
      "Epoch 14, loss: 1.0265415906906128, Epoch time: 69.67s\n",
      "Epoch 15, loss: 0.9885061979293823, Epoch time: 70.05s\n",
      "Epoch 16, loss: 0.9489414095878601, Epoch time: 70.14s\n",
      "Epoch 17, loss: 0.9110711216926575, Epoch time: 71.52s\n",
      "Epoch 18, loss: 0.874193012714386, Epoch time: 70.59s\n",
      "Epoch 19, loss: 0.8337652087211609, Epoch time: 72.76s\n",
      "Epoch 20, loss: 0.7989179491996765, Epoch time: 72.29s\n",
      "Epoch 21, loss: 0.7602517604827881, Epoch time: 76.48s\n",
      "Epoch 22, loss: 0.7113379240036011, Epoch time: 73.08s\n",
      "Epoch 23, loss: 0.6742400527000427, Epoch time: 71.91s\n",
      "Epoch 24, loss: 0.6339400410652161, Epoch time: 73.05s\n",
      "Epoch 25, loss: 0.5900057554244995, Epoch time: 74.63s\n",
      "Epoch 26, loss: 0.5539786219596863, Epoch time: 74.82s\n",
      "Epoch 27, loss: 0.501185953617096, Epoch time: 76.11s\n",
      "Epoch 28, loss: 0.449724018573761, Epoch time: 76.36s\n",
      "Epoch 29, loss: 0.40467724204063416, Epoch time: 74.61s\n",
      "Epoch 30, loss: 0.3676438629627228, Epoch time: 75.25s\n",
      "Epoch 31, loss: 0.31841954588890076, Epoch time: 74.86s\n",
      "Epoch 32, loss: 0.2787168622016907, Epoch time: 71.16s\n",
      "Epoch 33, loss: 0.23006108403205872, Epoch time: 70.90s\n",
      "Epoch 34, loss: 0.1934455931186676, Epoch time: 71.79s\n",
      "Epoch 35, loss: 0.15978218615055084, Epoch time: 72.30s\n",
      "Epoch 36, loss: 0.1333637684583664, Epoch time: 71.14s\n",
      "Epoch 37, loss: 0.10438773781061172, Epoch time: 70.94s\n",
      "Epoch 38, loss: 0.085811547935009, Epoch time: 71.09s\n",
      "Epoch 39, loss: 0.07120955735445023, Epoch time: 71.09s\n",
      "Epoch 40, loss: 0.061601560562849045, Epoch time: 71.14s\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    epoch_start_time = time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print(f'Epoch {ep}, loss: {total_loss/steps_per_epoch}, Epoch time: {time() - epoch_start_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(sentence):\n",
    "    return sentence.split('<s>')[-1].split('<e>')[0]\n",
    "\n",
    "def evaluate(sentence):\n",
    "    # preprocessing\n",
    "    sentence = re.sub(r\"[^a-z?.!,:]+\", \" \", sentence.lower())\n",
    "    sentence = re.sub(r\"([?.!,:])\", r\" \\1 \", sentence).strip()\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = '<s> ' + sentence + ' <e>'\n",
    "\n",
    "    inputs = [x_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_x, padding='post')\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([y_tokenizer.word_index['<s>']], 0)\n",
    "\n",
    "    for t in range(max_length_y):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += y_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if y_tokenizer.index_word[predicted_id] == '<e>':\n",
    "            return remove_tags(result), remove_tags(sentence)\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return remove_tags(result), remove_tags(sentence)\n",
    "\n",
    "def ask(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "\n",
    "    print(f'Question: {sentence}')\n",
    "    print(f'Predicted answer: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  hi , how are you doing ? \n",
      "Predicted answer: i m fine . how about yourself ? \n"
     ]
    }
   ],
   "source": [
    "ask('hi, how are you doing?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  are you good ? \n",
      "Predicted answer: i m average . \n"
     ]
    }
   ],
   "source": [
    "ask('Are you good ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  are you in school \n",
      "Predicted answer: i hope so . i hope so . i hope so . i hope so . i hope so . i hope so . \n"
     ]
    }
   ],
   "source": [
    "ask('Are you in school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  why do you mean ? \n",
      "Predicted answer: what do you mean , too . \n"
     ]
    }
   ],
   "source": [
    "ask('Why do you mean?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
